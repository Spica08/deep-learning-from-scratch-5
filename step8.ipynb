{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdWBIiiSFgLF8xK5kWdniG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spica08/deep-learning-from-scratch-5/blob/main/step8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s4KDOqWBpMhM"
      },
      "outputs": [],
      "source": [
        "# set up\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step8 拡散モデルの理論"
      ],
      "metadata": {
        "id": "keruFPzOpPKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 VAEから拡散モデルへ\n",
        "本書での「拡散モデル」は、「Denoising Diffusion Probabilistic Models」を指す。"
      ],
      "metadata": {
        "id": "HVQNPvs2pURd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.1 VAEの復習\n",
        "VAEでは、潜在変数を固定の正規分布よりサンプリングし、潜在変数から観測変数への変換をニューラルネットワークで行うことでデータ生成を行う。また、データxが得られた時の潜在変数zの事後分布を求めるためにもう1つ別のニューラルネットワークを使用する。"
      ],
      "metadata": {
        "id": "3h_gyvgCplFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.2 潜在変数の階層化\n",
        "これまで見てきたVAEは潜在変数の数が1つだったが、この潜在変数を階層化したモデルを階層型VAEという。  \n",
        "階層型VAEでは、直前の確率変数だけから決定される。この性質をマルコフ性といい、これを仮定することでパラメータの増加を防ぐ事ができる。そして潜在変数を階層化することでより複雑な表現を可能にする。  \n",
        "例えば階層をT層にすることを考えると、それぞれの階層でencoder, decoderが必要となり合計2T個のニューラルネットワークが必要になる。このままではTが大きくなった時に実現が困難になってしまう。これを解決するために進化したものが拡散モデルである。"
      ],
      "metadata": {
        "id": "u1wIlBXnqLZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.3 拡散モデルへ\n",
        "階層型VAEは以下の2点を変更するだけで「拡散モデル」となる。  \n",
        "1. 観測変数と潜在変数の次元を同じにする。  \n",
        "2. エンコーダは、固定の正規分布によるノイズを追加する  \n",
        "\n",
        "これに伴って、潜在変数の記号は全てxで統一し、エンコーダのパラメータが不要となる。そして、拡散モデルでは、ノイズを除去する過程をニューラルネットワークでモデル化する。"
      ],
      "metadata": {
        "id": "uDIJvswJuOUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 拡散過程と逆拡散過程\n",
        "拡散モデルには、1:ノイズを追加する**拡散過程**と、2:ノイズを除去する**逆拡散過程**が存在する。"
      ],
      "metadata": {
        "id": "Oqesr-_iu50G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.1 拡散過程\n",
        "拡散過程では、1つ前の時刻のデータに対してノイズを加える。このノイズの加え方に関して満たすべき条件は、「最終時刻における潜在変数$x_T$が完全なノイズになること」である。  \n",
        "\n",
        "ここで、$x_T$が完全なノイズ$\\mathcal{N}(x_T;0,I)$に従うように、各時刻で加えるノイズの大きさを考える。1つに以下の方法がある。  \n",
        "\\begin{equation}\n",
        "q(x_T | x_{T - 1}) = \\mathcal{N}(x_T;\\sqrt{1 - \\beta_t}x_{T - 1}, \\beta_tI)\n",
        "\\end{equation}\n",
        "tは各時刻を表し、$\\beta_t$はあらかじめ設定する0.01などの値とする。$\\beta_t$が大きいほど分散と共に加えるノイズが大きくなる。この値を時刻ごとにT個の値として設定する。この時Tを1000くらいにある程度大きくし各βを適切に設定(ノイズスケジューリングという)すれば、最終的なデータは$\\mathcal{N}(x_T;0,I)$に従う(後述)。\n",
        "\n",
        "\\begin{equation}\n",
        "q(x_T | x_{T - 1}) = \\mathcal{N}(x_T;\\sqrt{1 - \\beta_t}x_{T - 1}, \\beta_tI)\n",
        "\\end{equation}\n",
        "この式は、平均ベクトルが$\\sqrt{1 - \\beta_t}x_{T - 1}$、共分散行列が$\\beta_tI$の正規分布であり、サンプリングされる値は変数変換トリックを用いると\n",
        "\n",
        "\\begin{align}\n",
        "\\epsilon &\\backsim \\mathcal{N}(\\epsilon;0,I)\\\\\n",
        "x_T &= \\sqrt{1 - \\beta_t}x_{T - 1} + \\sqrt{\\beta_T}\\epsilon\n",
        "\\end{align}\n",
        "と表す事ができる。これは、標準正規分布より$\\epsilon$をサンプリングし、$\\sqrt{\\beta_T}$倍した値をノイズとして加えることを表す。つまり、前時刻のデータをややスケールダウンさせ、小さなノイズを加える、ということを繰り返す。"
      ],
      "metadata": {
        "id": "LC7H1mRkvJzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2.2 逆拡散過程\n",
        "逆拡散過程ではノイズを除去する処理をニューラルネットワークで行う。  \n",
        "\n",
        "拡散モデルでは全部でT回のノイズ除去を行う。各時刻において個別のニューラルネットワークを使用することも考えられるが、これではTが大きくなった時に計算が現実的ではなくなってしまう。  \n",
        "拡散モデルでは、$\\lbrace x_0, x_1, \\dots, x_n\\rbrace$の次元数が全て同じであるため、ニューラルネットワークの入出力の次元数も同じになる。これによりニューラルネットワークの構造を全て共通化できる。ここに、時刻tの情報を入力として与えることで、各時刻のノイズ除去を1つのニューラルネットワークで行う事ができる。  \n",
        "\n",
        "以上より、逆拡散過程の目的は、ニューラルネットワークを用いて$p_\\theta(x_{t - 1} | x_t)$をモデル化する事である。拡散過程が正規分布に従う場合、時間幅が十分小さければ$p_\\theta(x_{t - 1} | x_t)$も正規分布に従うが、確率分布を直接出力することはでできないので、出力を平均ベクトルとする正規分布だと考えてネットワークを構成する。数式で表すと以下の通り。\n",
        "\\begin{align}\n",
        "\\hat{x}_{t - 1} &= NeuralNet(x_t, t;\\theta)\\qquad 時刻tのデータをニューラルネットワークへ入力し\\\\\n",
        "p_\\theta(x_{t - 1} | x_t) &= \\mathcal{N}(x_{t - 1};\\hat{x}_{t - 1},I) \\qquad その出力を平均ベクトルとする正規分布から前の時刻のデータを得る。\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "1rKtMUT9xFD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3 ELBOの計算-1"
      ],
      "metadata": {
        "id": "c8Sd21210X3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3.1 拡散モデルのELBO\n",
        "拡散モデルのELBOはVAEと同様に導出できる。VAEのELBOは以下。\n",
        "\\begin{align}\n",
        "ELBO(x;\\theta, \\phi) &= \\int q_\\phi(z | x)log\\frac{p_\\theta(x, z)}{q_\\phi(z | x)}dz\\\\\n",
        "                     &= \\mathbb{E}_{q_\\phi(z | x)}\\left[log\\frac{p_\\theta(x, z)}{q_\\phi(z | x)}\\right]\n",
        "\\end{align}\n",
        "3点変更して拡散モデルのELBOとする。変更点は以下の通り。\n",
        "1. $x$を$x_0$に変更  \n",
        "2. z(潜在変数)を$\\lbrace x_1, x_2, \\dots, x_T\\rbrace$へ変更  \n",
        "3. パラメータ$\\phi$(エンコーダのパラメータ)を消去\n",
        "\n",
        "変更したELBOは以下の通り。\n",
        "\\begin{align}\n",
        "ELBO(x;\\theta, \\phi) &= \\mathbb{E}_{q(x_1, x_2, \\dots, x_T | x_0)}\\left[log\\frac{p_\\theta(x_0, x_1, x_2, \\dots, x_T)}{q(x_1, x_2, \\dots, x_T | x_0)}\\right]\\\\\n",
        "                     &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\frac{p_\\theta(x_0:x_T)}{q(x_1:x_T | x_0)}\\right]\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "J_oNkaiJ0eoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3.2 ELBOの式展開\n",
        "先ほど考えたELBOに出てくる$p_\\theta(x_0:x_T)$について考えると、乗法定理とマルコフ性により以下のように変形できる。\n",
        "\\begin{align}\n",
        "p_\\theta(x_0:x_T) &= p_\\theta(x_0|x_1)p_\\theta(x_1|x_2)\\dots p_\\theta(x_{T - 1}|x_T)p(x_T)\\\\\n",
        "                   &= p(x_T) \\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t)\n",
        "\\end{align}\n",
        "同様に$q(x_1:x_T | x_0)$は、\n",
        "\\begin{align}\n",
        "q(x_1:x_T | x_0) &= \\prod_{t = 1}^T q(x_{t} | x_{t - 1})\n",
        "\\end{align}\n",
        "となる。よってELBOは、\n",
        "\\begin{align}\n",
        "ELBO(x;\\theta, \\phi) &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\frac{p_\\theta(x_0:x_T)}{q(x_1:x_T | x_0)}\\right]\\\\\n",
        "                     &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\frac{p(x_T) \\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t)}{\\prod_{t = 1}^T q(x_{t} | x_{t - 1})}\\right]\\\\\n",
        "                     &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t) + log\\frac{p(x_T)}{\\prod_{t = 1}^T q(x_{t} | x_{t - 1})}\\right]\n",
        "\\end{align}\n",
        "$\\theta$を含まない項のみを考慮し新たに目的関数$J(\\theta)$とすると、\n",
        "\\begin{align}\n",
        "J(\\theta) &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t)\\right]\\\\\n",
        "          &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[\\sum_{t = 1}^T logp_\\theta(x_{t - 1} | x_t)\\right]\n",
        "\\end{align}\n",
        "この目的関数は期待値で表されるので、モンテカルロ法で近似する。サンプルサイズを1とすると、\n",
        "\\begin{align}\n",
        "x_1:x_T &\\backsim q(x_1:x_T | x_0)\\\\\n",
        "J(\\theta) &\\approx \\sum_{t = 1}^T logp_\\theta(x_{t - 1} | x_t)\n",
        "\\end{align}\n",
        "この時の$P_\\theta(x_{t - 1}|x_t)$は、\n",
        "\\begin{align}\n",
        "\\hat{x}_{t - 1} &= NeuralNet(x_t, t;\\theta)\\\\\n",
        "p_\\theta(x_{t - 1} | x_t) &= \\mathcal{N}(x_{t - 1};\\hat{x}_{t - 1},I)\n",
        "\\end{align}\n",
        "よって目的関数は、\n",
        "\\begin{align}\n",
        "J(\\theta) &\\approx \\sum_{t = 1}^T logp_\\theta(x_{t - 1} | x_t)\\\\\n",
        "          &= \\sum_{t = 1}^T log\\mathcal{N}(x_{t - 1};\\hat{x}_{t - 1},I)\\\\\n",
        "          &= \\sum_{t = 0}^{T - 1} log\\mathcal{N}(x_t;\\hat{x}_t,I)\\\\\n",
        "          &= \\sum_{t = 0}^{T - 1} log \\left(\\frac{1}{\\sqrt{(2\\pi)^D|I|}}exp\\left(-\\frac{1}{2}(x_t - \\hat{x_t})^TI^{-1}(x_t - \\hat{x_t}) \\right)\\right)\\\\\n",
        "          &= \\sum_{t = 0}^{T - 1} \\left(-\\frac{1}{2}(x_t - \\hat{x_t})^TI^{-1}(x_t - \\hat{x_t}) \\right) + \\sum_{t = 0}^{T - 1} log\\left(\\frac{1}{\\sqrt{(2\\pi)^D|I|}}\\right)\\\\\n",
        "          &= -\\frac{1}{2}\\sum_{t = 0}^{T - 1}\\left(x_t - \\hat{x_t})^T(x_t - \\hat{x_t}\\right) + Tlog\\left(\\frac{1}{\\sqrt{(2\\pi)^D|I|}}\\right)\n",
        "\\end{align}\n",
        "定数項を無視して、\n",
        "\\begin{align}\n",
        "J(\\theta) &= -\\frac{1}{2}\\sum_{t = 0}^{T - 1}\\left(x_t - \\hat{x_t})^T(x_t - \\hat{x_t}\\right)\\\\\n",
        "          &= -\\frac{1}{2}\\sum_{t = 0}^{T - 1} ||x_t - \\hat{x_t}||^2\n",
        "\\end{align}\n",
        "\n",
        "以上より、拡散モデルのELBOから損失関数を導出すると、  \n",
        "1. 拡散過程によりT個のサンプリングを行い  \n",
        "2. ニューラルネットワークをT回適用してノイズ除去を行い  \n",
        "3. 各時刻における潜在変数とサンプリング結果との2乗誤差を求める。  \n",
        "\n",
        "この方法では、拡散過程をT回行うときサンプリングもT回行う必要があり、計算量が大きくなるという課題がある。\n"
      ],
      "metadata": {
        "id": "7voPQpg021NS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 ELBOの計算-2\n",
        "前節では、ELBOをT個のサンプリングデータによって近似した。このままでは計算量が非常に大きいため、続いて2個のサンプリングデータで近似する方法について考える。  \n",
        "\n",
        "ここで重要になるのが、拡散過程で$q(x_t | x_0)$が解析的に表せるということである。つまり、元データ$x_0$に対してノイズを一度だけ追加することで任意の時間tにおける$x_t$をサンプリングする事ができる。"
      ],
      "metadata": {
        "id": "iK8MgW1kuptY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4.1 $q(x_t | x_0)$の式\n",
        "\\begin{equation}\n",
        "q(x_T | x_{T - 1}) = \\mathcal{N}(x_T;\\sqrt{1 - \\beta_t}x_{T - 1}, \\beta_tI)\n",
        "\\end{equation}\n",
        "先ほどまでは上記の式に従って拡散過程ではT回の処理を行なっていた。これを1度の処理で済むように$q(x_t | x_0)$を解析的に表す必要がある。  \n",
        "\n",
        "結果を先に示すと、\n",
        "\\begin{equation}\n",
        "q(x_t | x_0) = \\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha_t}}x_{0}, (1 - \\bar{\\alpha}_t)I)\n",
        "\\end{equation}\n",
        "\\begin{align}\n",
        "\\alpha_t &= 1 - \\beta_t\\\\\n",
        "\\bar{\\alpha_t} &= \\alpha_t \\alpha_{t - 1} \\dots \\alpha_1\n",
        "\\end{align}\n",
        "となる。この式から、時刻Tにおける$x_T$が完全なノイズになることも確認する事ができる。"
      ],
      "metadata": {
        "id": "guHR1WJPvLLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4.2 ELBOの近似解\n",
        "前節までで考えてたELBOおよび目的関数は以下の通り。\n",
        "\\begin{align}\n",
        "ELBO(x;\\theta, \\phi) &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\frac{p_\\theta(x_0:x_T)}{q(x_1:x_T | x_0)}\\right]\\\\\n",
        "                     &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\frac{p(x_T) \\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t)}{\\prod_{t = 1}^T q(x_{t} | x_{t - 1})}\\right]\\\\\n",
        "                     &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t) + log\\frac{p(x_T)}{\\prod_{t = 1}^T q(x_{t} | x_{t - 1})}\\right]\\\\\n",
        "J(\\theta) &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[log\\prod_{t = 1}^T p_\\theta(x_{t - 1} | x_t)\\right]\\\\\n",
        "          &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[\\sum_{t = 1}^T logp_\\theta(x_{t - 1} | x_t)\\right]\n",
        "\\end{align}\n",
        "この目的関数は以下のように展開する事ができる。\n",
        "\\begin{align}\n",
        "J(\\theta) &= \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[\\sum_{t = 1}^T logp_\\theta(x_{t - 1} | x_t)\\right]\\\\\n",
        "          &= \\sum_{t = 1}^T \\mathbb{E}_{q(x_1:x_T | x_0)}\\left[logp_\\theta(x_{t - 1} | x_t)\\right]\\qquad \\because 期待値の線形性\\\\\n",
        "          &= \\sum_{t = 1}^T \\mathbb{E}_{q(x_{t - 1}, x_t | x_0)}\\left[logp_\\theta(x_{t - 1} | x_t)\\right]\\qquad \\because 関連する変数の期待値\\\\\n",
        "\\end{align}\n",
        "$q(x_{t - 1}, x_t | x_0)$からのサンプリングは2つの手順で生成する事ができる(1 -> $q(x_{t - 1} | x_0)$から$x_{t - 1}$をサンプリング、2 -> $q(x_t | x_{t - 1})$から$x_t$をサンプリング)。  \n",
        "\n",
        "このままでは和の計算を解消しきれていないが、$\\sum$を前に出したことによって和の計算を一様分布に関する期待値として表す事ができるようになった。  \n",
        "\n",
        "一様分布$u(t)$について、任意の関数f(x)の期待値を考えると、\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{u(t)}[f(x)] &= \\sum_{t = 1}^T u(t)f(t)\\\\\n",
        "                       &= \\frac{1}{T}\\sum_{t = 1}^Tf(t)\\\\\n",
        "よって\\\\\n",
        "\\sum_{t = 1}^Tf(t) &= T\\mathbb{E}_{u(t)}[f(t)]\n",
        "\\end{align}\n",
        "以上より、任意の関数f(t)について1~TのT個の和は、一様分布$u(t)$に関する期待値として表す事ができる。先ほどの目的関数にこれを適用して、\n",
        "\\begin{align}\n",
        "J(\\theta) &= \\sum_{t = 1}^T \\mathbb{E}_{q(x_{t - 1}, x_t | x_0)}\\left[logp_\\theta(x_{t - 1} | x_t)\\right]\\\\\n",
        "          &= T\\mathbb{E}_{u(t)}[\\mathbb{E}_{q(x_{t - 1}, x_t | x_0)}\\left[logp_\\theta(x_{t - 1} | x_t)\\right]]\n",
        "\\end{align}\n",
        "目的関数を期待値をして表す事ができたので、モンテカルロ法による近似を行う事ができ和の計算の必要がなくなった。具体的には以下のように近似を行う。  \n",
        "\n",
        "\\begin{align}\n",
        "t &\\backsim \\mathcal{U}\\lbrace1, T\\rbrace\\\\\n",
        "x_{t - 1} &\\backsim q(x_{t - 1} | x_0)\\\\\n",
        "x_t &\\backsim q(x_t | x_{t - 1})\\\\\n",
        "J(\\theta) &\\approx Tlogp_\\theta(x_{t - 1} | x_t)\\\\\n",
        "          &= -\\frac{T}{2} ||x_{t - 1} - \\hat{x_{t - 1}}||^2\n",
        "\\end{align}\n",
        "\n",
        "以上より、2回のサンプリングを利用した目的関数の計算では、\n",
        "1. 一様分布から任意の時刻tをサンプリングする  \n",
        "2. 2段階のサンプリングにより$x_{t - 1}, x_t$をサンプリングする\n",
        "2. ニューラルネットワークに$x_t$を入力して$x_{t - 1}$を出力する  \n",
        "3. 時刻t - 1における潜在変数とサンプリング結果との2乗誤差を求める。  "
      ],
      "metadata": {
        "id": "RAjCVfQZwrHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4.3 $q(x_t | x_0)$の導出\n",
        "拡散過程では元データに対してガウスノイズをT回付与する。ここでガウスノイズの重要な性質として、「ガウスノイズの和もガウスノイズになる」というものがある。  \n",
        "具体的には、$\\mathcal{N_x}(\\mu_x, \\sigma_x^2), \\mathcal{N_y}(\\mu_y, \\sigma_y^2)$があるとき、独立にサンプリングした変数x, yの和をzとすると、zは平均$\\mu_x + \\mu_y$、分散$\\sigma_x^2 + \\sigma_y^2$の正規分布に従う。  \n",
        "これを拡散過程に適用する。拡散過程の各時刻でのノイズは以下\n",
        "\\begin{equation}\n",
        "q(x_t | x_{t - 1}) = \\mathcal{N}(x_t;\\sqrt{1 - \\beta_t}x_{t - 1}, \\beta_tI)\n",
        "\\end{equation}\n",
        "$\\alpha_t = 1 - \\beta_t$とすると、\n",
        "\\begin{equation}\n",
        "q(x_t | x_{t - 1}) = \\mathcal{N}(x_t;\\sqrt{\\alpha_t}x_{t - 1}, (1 - \\alpha_t)I)\n",
        "\\end{equation}\n",
        "このサンプリングを変数変換トリックを使用して表すと、\n",
        "\\begin{align}\n",
        "\\epsilon_t &\\backsim \\mathcal{N}(\\epsilon_t;0, I)\\\\\n",
        "x_t &= \\sqrt{\\alpha_t}x_{t - 1} + \\epsilon_t \\sqrt{1 - \\alpha_t}\n",
        "\\end{align}\n",
        "時刻t - 1を考えるためにtにt - 1を代入して、\n",
        "\\begin{align}\n",
        "\\epsilon_{t - 1} &\\backsim \\mathcal{N}(\\epsilon_{t - 1};0, I)\\\\\n",
        "x_{t - 1} &= \\sqrt{\\alpha_{t - 1}}x_{t - 2} + \\epsilon_{t - 1} \\sqrt{1 - \\alpha_{t - 1}}\n",
        "\\end{align}\n",
        "これらを合わせて以下の式を得る。\n",
        "\\begin{align}\n",
        "x_t &= \\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t - 1}}x_{t - 2} + \\epsilon_{t - 1} \\sqrt{1 - \\alpha_{t - 1}}) + \\epsilon_t \\sqrt{1 - \\alpha_t}\\\\\n",
        "    &= \\sqrt{\\alpha_t}\\sqrt{\\alpha_{t - 1}}x_{t - 2} + \\sqrt{\\alpha_t}\\epsilon_{t - 1} \\sqrt{1 - \\alpha_{t - 1}} + \\epsilon_t \\sqrt{1 - \\alpha_t}\\\\\n",
        "    &= \\sqrt{\\alpha_t}\\sqrt{\\alpha_{t - 1}}x_{t - 2} + \\sqrt{\\alpha_t - \\alpha_t\\alpha_{t - 1}} \\epsilon_{t - 1} + \\sqrt{1 - \\alpha_t}\\epsilon_t\n",
        "\\end{align}\n",
        "$\\epsilon_t, \\epsilon_{t - 1}$は共に正規分布から独立に生成されたサンプルであるため、$\\sqrt{\\alpha_t - \\alpha_t\\alpha_{t - 1}} \\epsilon_{t - 1} + \\sqrt{1 - \\alpha_t}\\epsilon_t$ も1つの正規分布として表す事ができる。$\\epsilon_t, \\epsilon_{t - 1}$の平均、分散をそれぞれ考慮すると、\n",
        "\\begin{align}\n",
        "\\sqrt{\\alpha_t - \\alpha_t\\alpha_{t - 1}} \\epsilon_{t - 1} + \\sqrt{1 - \\alpha_t}\\epsilon_t &\\backsim \\mathcal{N}(0, (\\sqrt{\\alpha_t - \\alpha_t\\alpha_{t - 1}})^2I + (\\sqrt{1 - \\alpha_t})^2I)\\\\\n",
        "&\\backsim \\mathcal{N}(0, (1 - \\alpha_t \\alpha_{t - 1})I)\n",
        "\\end{align}\n",
        "そのため$x_t$のサンプリングはt - 2時点でのデータとパラメータを使用して\n",
        "\\begin{align}\n",
        "\\epsilon_t &\\backsim \\mathcal{N}(\\epsilon_t;0, I)\\\\\n",
        "x_t &= \\sqrt{\\alpha_t \\alpha_{t - 1}}x_{t - 2} + \\sqrt{1 - \\alpha_t \\alpha_{t - 1}}\\epsilon_t\n",
        "\\end{align}\n",
        "と表す事ができる。これを繰り返すと、\n",
        "\\begin{align}\n",
        "q(x_t | x_0) &= \\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha_t}}x_{0}, (1 - \\bar{\\alpha}_t)I)\\\\\n",
        "\\alpha_t &= 1 - \\beta_t\\\\\n",
        "\\bar{\\alpha_t} &= \\alpha_t \\alpha_{t - 1} \\dots \\alpha_1\n",
        "\\end{align}\n",
        "が得られる。"
      ],
      "metadata": {
        "id": "fAEuiV7S3bOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5 ELBOの計算-3\n",
        "最後の改善として1つのサンプリングからELBOを近似する方法を導出する。  \n",
        "重要となるのが$q(x_{t - 1}|x_t, x_0)$という確率分布で、これは解析的に表すことが可能である。"
      ],
      "metadata": {
        "id": "9Q81CTAx9qGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5.1 $q(x_{t - 1} |x_t, x_0)$の式(結果)\n",
        "先ほど同様先に結果を示すと、\n",
        "\\begin{equation}\n",
        "q(x_{t - 1} |x_t, x_0) = \\mathcal{N}(x_{t - 1};\\mu_q(x_t, x_0), \\sigma_q^2(t)I)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{align}\n",
        "\\alpha_t &= 1 - \\beta_t\\\\\n",
        "\\bar{\\alpha_t} &= \\alpha_t \\alpha_{t - 1} \\dots \\alpha_1\\\\\n",
        "\\mu_q(x_t, x_0) &= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t - 1})x_t + \\sqrt{\\bar{\\alpha}_{t - 1}}(1 - \\alpha_t)x_0}{1 - \\bar{\\alpha_t}}\\\\\n",
        "\\sigma_q^2(t) &= \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t - 1})}{1 - \\bar{\\alpha_t}}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "LB3wQ9Lg9_Cx"
      }
    }
  ]
}